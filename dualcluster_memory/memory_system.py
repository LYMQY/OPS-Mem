import keyword
from typing import List, Dict, Optional, Any, Tuple
import uuid
from datetime import datetime
from .llm_controller import LLMController
from .retrievers import ChromaRetriever
import json
import logging
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import os
from abc import ABC, abstractmethod
from transformers import AutoModel, AutoTokenizer
from nltk.tokenize import word_tokenize
import pickle
from pathlib import Path
import time

# æ—¥å¿—é…ç½®
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
logger = logging.getLogger("DualClusterMemorySystem")

class MemoryNode:
    """Case Layerï¼ˆL1ï¼‰- å…·ä½“é—®é¢˜çš„å®Œæ•´æ±‚è§£è®°å½•"""
    def __init__(self,
                 problem_description: Optional[str] = None,
                 modeling_logic: Optional[str] = None,
                 key_constraint_snippets: Optional[str] = None,
                 full_code: Optional[str] = None,
                 modeling_cluster_id: Optional[str] = None,
                 implementation_cluster_id: Optional[str] = None,
                 id: Optional[str] = None,
                 timestamp: Optional[str] = None):
        """åˆå§‹åŒ–è®°å¿†èŠ‚ç‚¹ï¼Œå­˜å‚¨é—®é¢˜-æ¨¡å‹-ä»£ç å…¨æµç¨‹ä¿¡æ¯"""
        # åŸºç¡€æ ‡è¯†
        self.id = id or str(uuid.uuid4())  # å”¯ä¸€ID
        self.timestamp = timestamp or datetime.now().strftime("%Y-%m-%d %H:%M:%S")  # åˆ›å»ºæ—¶é—´
        self.status = "pending"  # çŠ¶æ€ï¼špending(å¾…æ•´åˆ)/integrated(å·²æ•´åˆ)

        # æ ¸å¿ƒå†…å®¹ï¼ˆé—®é¢˜-æ¨¡å‹-ä»£ç ï¼‰
        self.problem_description = problem_description or "General Problem"  # é—®é¢˜æè¿°
        self.modeling_logic = modeling_logic or "General Modeling Logic"  # å»ºæ¨¡é€»è¾‘ï¼ˆè‡ªç„¶è¯­è¨€ï¼‰
        self.key_constraint_snippets = key_constraint_snippets or "General Key Constraint Snippets"  # å…³é”®çº¦æŸä»£ç ç‰‡æ®µ
        self.full_code = full_code or "General Full Code"  # å®Œæ•´å®ç°ä»£ç 

        # åŒç°‡å…³è”
        self.modeling_cluster_id = modeling_cluster_id  # æ‰€å±å»ºæ¨¡ç°‡ID
        self.implementation_cluster_id = implementation_cluster_id  # æ‰€å±å®ç°ç°‡ID
        
        # åµŒå…¥å‘é‡ï¼ˆå»¶è¿Ÿç”Ÿæˆï¼Œæ·»åŠ æ—¶è®¡ç®—ï¼‰
        self.modeling_embedding: Optional[np.ndarray] = None  # å»ºæ¨¡é€»è¾‘åµŒå…¥
        self.implementation_embedding: Optional[np.ndarray] = None  # å®ç°ä»£ç åµŒå…¥

        # æ£€ç´¢ä¸æ¼”åŒ–ç›¸å…³å­—æ®µ
        self.retrieval_count = 0  # æ£€ç´¢æ¬¡æ•°ï¼ˆç”¨äºæƒé‡è°ƒæ•´ï¼‰
        self.links = []  # å…³è”çš„å…¶ä»–è®°å¿†èŠ‚ç‚¹ID
        self.evolution_history = []  # æ¼”åŒ–è®°å½•


class DualClusterMemorySystem:
    """åŒç°‡è®°å¿†ç³»ç»Ÿï¼šåŸºäºMemoryNodeå®ç°å»ºæ¨¡/å®ç°åŒç°‡ç®¡ç†"""
    def __init__(self, 
                 model_name: str = 'all-MiniLM-L6-v2',
                 llm_backend: str = "openai",
                 llm_model: str = "gpt-4o-mini",
                 evo_threshold: int = 10,  # ç°‡æ›´æ–°é˜ˆå€¼ï¼šç´¯è®¡10ä¸ªpendingèŠ‚ç‚¹è§¦å‘æ•´åˆ
                 similarity_threshold: float = 0.3,  # ç°‡å½’å±é˜ˆå€¼ï¼šç›¸ä¼¼åº¦<0.3å½’ä¸ºå·²æœ‰ç°‡
                 api_key: Optional[str] = None):  
        """åˆå§‹åŒ–ç³»ç»Ÿï¼šåˆ›å»ºåŒç°‡æ£€ç´¢å™¨ã€LLMæ§åˆ¶å™¨ã€æ¼”åŒ–å‚æ•°"""
        # æœ¬åœ°è®°å¿†å­˜å‚¨ï¼ˆkey: MemoryNode.id, value: MemoryNodeå®ä¾‹ï¼‰
        self.memories: Dict[str, MemoryNode] = {}
        self.model_name = model_name

        # 1. åˆå§‹åŒ–ChromaDBåŒç°‡æ£€ç´¢å™¨ï¼ˆå»ºæ¨¡ç°‡+å®ç°ç°‡ï¼‰+ å…¨é‡è®°å¿†æ£€ç´¢å™¨
        try:
            # é‡ç½®æ—§é›†åˆï¼Œç¡®ä¿æ•°æ®å¹²å‡€
            for coll_name in ["memories", "model", "implementation"]:
                temp_retriever = ChromaRetriever(collection_name=coll_name, model_name=model_name)
                temp_retriever.client.reset()
            logger.info("ChromaDBæ—§é›†åˆé‡ç½®å®Œæˆ")
        except Exception as e:
            logger.warning(f"ChromaDBé‡ç½®å¤±è´¥ï¼Œä½¿ç”¨æ–°é›†åˆï¼š{str(e)}")
        
        # åˆ›å»ºæ–°æ£€ç´¢å™¨å®ä¾‹
        self.full_retriever = ChromaRetriever(collection_name="memories", model_name=model_name)  # å…¨é‡è®°å¿†
        self.model_retriever = ChromaRetriever(collection_name="model", model_name=model_name)    # å»ºæ¨¡ç°‡
        self.implementation_retriever = ChromaRetriever(collection_name="implementation", model_name=model_name)  # å®ç°ç°‡

        # 2. åˆå§‹åŒ–LLMæ§åˆ¶å™¨ï¼ˆç”¨äºå†…å®¹åˆ†æã€æ¼”åŒ–å†³ç­–ï¼‰
        self.llm_controller = LLMController(llm_backend, llm_model, api_key)

        # 3. æ¼”åŒ–ä¸æ£€ç´¢å‚æ•°
        self.evo_cnt = 0  # å¾…æ•´åˆèŠ‚ç‚¹è®¡æ•°å™¨
        self.evo_threshold = evo_threshold  # æ¼”åŒ–è§¦å‘é˜ˆå€¼
        self.similarity_threshold = similarity_threshold  # ç°‡å½’å±ç›¸ä¼¼åº¦é˜ˆå€¼
        self.embedding_model = SentenceTransformer(model_name)  # ç”¨äºç”ŸæˆåµŒå…¥å‘é‡

        # 4. æ¼”åŒ–å†³ç­–Promptï¼ˆå›ºå®šæ¨¡æ¿ï¼Œå¼•å¯¼LLMè¾“å‡ºç»“æ„åŒ–ç»“æœï¼‰
        self._evolution_system_prompt = """
        ä½ æ˜¯è®°å¿†æ¼”åŒ–å†³ç­–åŠ©æ‰‹ï¼Œéœ€è¦åˆ¤æ–­æ–°è®°å¿†èŠ‚ç‚¹æ˜¯å¦éœ€è¦ä¸å·²æœ‰èŠ‚ç‚¹æ•´åˆï¼Œå¹¶è¾“å‡ºå…·ä½“åŠ¨ä½œã€‚
        æ–°è®°å¿†ä¿¡æ¯ï¼š
        - å†…å®¹ï¼š{content}
        - ä¸Šä¸‹æ–‡ï¼š{context}
        - å…³é”®è¯ï¼š{keywords}
        
        ç›¸ä¼¼é‚»å±…è®°å¿†ï¼š
        {nearest_neighbors_memories}
        
        è¯·æŒ‰ä»¥ä¸‹è§„åˆ™å†³ç­–ï¼š
        1. è‹¥æ–°èŠ‚ç‚¹ä¸é‚»å±…ç›¸ä¼¼åº¦é«˜ï¼ˆä¸»é¢˜/æ–¹æ³•ä¸€è‡´ï¼‰ï¼Œshould_evolveè®¾ä¸ºtrueï¼Œå¦åˆ™falseï¼›
        2. åŠ¨ä½œï¼ˆactionsï¼‰å¯é€‰ï¼šstrengthenï¼ˆå¼ºåŒ–æ–°èŠ‚ç‚¹å…³è”ï¼‰ã€update_neighborï¼ˆæ›´æ–°é‚»å±…å…ƒæ•°æ®ï¼‰ï¼›
        3. è¾“å‡ºJSONæ ¼å¼ï¼Œä¸¥æ ¼éµå¾ªschemaï¼Œä¸é¢å¤–æ·»åŠ å†…å®¹ã€‚
        """

    def analyze_content(self, content: str) -> Dict:            
        """ç”¨LLMåˆ†æè¾“å…¥æ–‡æœ¬ï¼Œæå–MemoryNodeæ‰€éœ€çš„æ ¸å¿ƒå­—æ®µï¼ˆé—®é¢˜/æ¨¡å‹/ä»£ç ï¼‰"""
        prompt = """
        åˆ†æä»¥ä¸‹æ–‡æœ¬ï¼Œæå–4ä¸ªæ ¸å¿ƒå­—æ®µï¼Œè¾“å‡ºJSONæ ¼å¼ï¼š
        1. descriptionï¼šé—®é¢˜æè¿°ï¼ˆ1-2å¥è¯æ€»ç»“å¾…è§£å†³çš„é—®é¢˜ï¼‰ï¼›
        2. modelingï¼šå»ºæ¨¡é€»è¾‘ï¼ˆåŒ…å«å‡è®¾ã€å‚æ•°ã€ç›®æ ‡å‡½æ•°ã€çº¦æŸæ¡ä»¶ï¼‰ï¼›
        3. implementationï¼šå…³é”®çº¦æŸä»£ç ç‰‡æ®µï¼ˆä¸ä¸šåŠ¡çº¦æŸç›¸å…³çš„ä»£ç ï¼Œå¦‚if/foré€»è¾‘ï¼‰ï¼›
        4. codeï¼šå®Œæ•´å®ç°ä»£ç ï¼ˆå¯è¿è¡Œçš„å®Œæ•´ä»£ç ï¼ŒåŒ…å«å¯¼å…¥ã€å‡½æ•°å®šä¹‰ï¼‰ã€‚
        
        æ–‡æœ¬å†…å®¹ï¼š{content}
        
        JSONæ ¼å¼è¦æ±‚ï¼š
        {{
            "description": "é—®é¢˜æè¿°",
            "modeling": "å»ºæ¨¡é€»è¾‘",
            "implementation": "å…³é”®çº¦æŸä»£ç ",
            "code": "å®Œæ•´ä»£ç "
        }}
        """.format(content=content)

        try:
            # è°ƒç”¨LLMå¹¶æŒ‡å®šJSONè¾“å‡ºæ ¼å¼
            response = self.llm_controller.llm.get_completion(
                prompt,
                response_format={
                    "type": "json_schema",
                    "json_schema": {
                        "name": "memory_extraction",
                        "schema": {
                            "type": "object",
                            "properties": {
                                "description": {"type": "string"},
                                "modeling": {"type": "string"},
                                "implementation": {"type": "string"},
                                "code": {"type": "string"}
                            },
                            "required": ["description", "modeling", "implementation", "code"],
                            "additionalProperties": False
                        },
                        "strict": True
                    }
                }
            )
            return json.loads(response)
        except Exception as e:
            logger.error(f"LLMå†…å®¹åˆ†æå¤±è´¥ï¼š{str(e)}")
            # å¤±è´¥æ—¶è¿”å›é»˜è®¤å€¼ï¼Œé¿å…æµç¨‹ä¸­æ–­
            return {
                "description": "General Problem",
                "modeling": "General Modeling Logic",
                "implementation": "# No Key Constraint Code",
                "code": "# No Full Implementation Code"
            }

    def _generate_embeddings(self, text: str) -> np.ndarray:
        """ç”Ÿæˆæ–‡æœ¬çš„åµŒå…¥å‘é‡ï¼ˆç”¨SentenceTransformeræ¨¡å‹ï¼‰"""
        return self.embedding_model.encode(text, convert_to_tensor=False)

    def _assign_cluster(self, node: MemoryNode) -> Tuple[Optional[str], Optional[str]]:
        """ä¸ºæ–°èŠ‚ç‚¹åˆ†é…ç°‡ï¼šåŸºäºç›¸ä¼¼åº¦åŒ¹é…å·²æœ‰ç°‡ï¼Œå¦åˆ™åˆ›å»ºæ–°ç°‡ï¼ˆå«ç°‡ä¸­å¿ƒå’Œæ¨¡å¼æ€»ç»“ï¼‰"""
        modeling_cluster_id = None
        implementation_cluster_id = None

        # 1. å»ºæ¨¡ç°‡åˆ†é…ï¼šåŸºäºå»ºæ¨¡é€»è¾‘çš„æŠ½è±¡æ€»ç»“åŒ¹é…
        if node.modeling_logic.strip():
            # ç”ŸæˆåŸå§‹å»ºæ¨¡é€»è¾‘åµŒå…¥ï¼ˆç”¨äºèŠ‚ç‚¹è‡ªèº«ï¼‰
            node.modeling_embedding = self._generate_embeddings(node.modeling_logic)
            
            # æ£€ç´¢ç›¸ä¼¼å»ºæ¨¡ç°‡ï¼ˆk=1ï¼Œå–æœ€ç›¸ä¼¼ï¼‰
            model_results = self.model_retriever.search(query=node.modeling_logic, k=1)
            #logger.info(f"{model_results}")
            
            if model_results["distances"] and len(model_results["distances"][0]) > 0:
                similarity = 1 / (1 + model_results["distances"][0][0])  # è·ç¦»è½¬ç›¸ä¼¼åº¦
                if similarity <= self.similarity_threshold:
                    # ç›¸ä¼¼åº¦è¾¾æ ‡ï¼šåˆ†é…å·²æœ‰ç°‡IDï¼Œå¹¶æ›´æ–°å…¸å‹æ¡ˆä¾‹
                    modeling_cluster_id = model_results["ids"][0][0]
                    # è·å–ç°‡å…ƒæ•°æ®å¹¶æ›´æ–°å…¸å‹æ¡ˆä¾‹åˆ—è¡¨
                    cluster_metadata = model_results["metadatas"][0][0]
                    updated_node_ids = cluster_metadata["å…¸å‹æ¡ˆä¾‹"] + [node.id]
                    # é‡æ–°æ·»åŠ ç°‡ï¼ˆæ›´æ–°å…ƒæ•°æ®ï¼‰
                    self.model_retriever.delete_document(modeling_cluster_id)
                    self.model_retriever.add_cluster(
                        cluster=cluster_metadata["ç°‡ä¸­å¿ƒ"],  # ä¿æŒåŸç°‡ä¸­å¿ƒæ€»ç»“
                        metadata={
                            "type": "modeling",
                            "ç°‡ä¸­å¿ƒ": cluster_metadata["ç°‡ä¸­å¿ƒ"],
                            "æ¨¡å¼è¯¦ç»†æ€»ç»“": cluster_metadata["æ¨¡å¼è¯¦ç»†æ€»ç»“"],
                            "å…¸å‹æ¡ˆä¾‹": updated_node_ids
                        },
                        cluster_id=modeling_cluster_id
                    )
            elif modeling_cluster_id == None:
                    # ç›¸ä¼¼åº¦ä¸è¾¾æ ‡ï¼šåˆ›å»ºæ–°ç°‡ï¼ˆç”Ÿæˆç°‡ä¸­å¿ƒå’Œæ¨¡å¼æ€»ç»“ï¼‰
                    modeling_cluster_id = str(uuid.uuid4())
                    # è°ƒç”¨LLMç”Ÿæˆç°‡ä¸­å¿ƒæ€»ç»“å’Œæ¨¡å¼è¯¦ç»†æè¿°
                    cluster_summary = self._generate_modeling_cluster_summary(node.modeling_logic)
                    # ç”Ÿæˆç°‡ä¸­å¿ƒçš„æŠ½è±¡åµŒå…¥ï¼ˆç”¨äºåç»­ç°‡é—´åŒ¹é…ï¼‰
                    cluster_embedding = self._generate_embeddings(cluster_summary["ç°‡ä¸­å¿ƒ"])
                    # å­˜å…¥æ–°ç°‡
                    self.model_retriever.add_cluster(
                        cluster=cluster_summary["ç°‡ä¸­å¿ƒ"],  # ç”¨ç°‡ä¸­å¿ƒæ€»ç»“ä½œä¸ºæ£€ç´¢æ–‡æœ¬
                        metadata={
                            "type": "modeling",
                            "ç°‡ä¸­å¿ƒ": cluster_summary["ç°‡ä¸­å¿ƒ"],
                            "æ¨¡å¼è¯¦ç»†æ€»ç»“": cluster_summary["æ¨¡å¼è¯¦ç»†æ€»ç»“"],
                            "å…¸å‹æ¡ˆä¾‹": [node.id]  # åˆå§‹å…¸å‹æ¡ˆä¾‹ä¸ºå½“å‰èŠ‚ç‚¹
                        },
                        cluster_id=modeling_cluster_id
                    )
                    # å­˜å‚¨ç°‡ä¸­å¿ƒåµŒå…¥ï¼ˆç”¨äºèŠ‚ç‚¹ä¸ç°‡çš„ç²¾ç¡®åŒ¹é…ï¼‰
                    node.modeling_cluster_embedding = cluster_embedding

        # 2. å®ç°ç°‡åˆ†é…ï¼šåŸºäºä»£ç å®ç°çš„æŠ½è±¡æ€»ç»“åŒ¹é…
        if node.full_code.strip():
            # ç”ŸæˆåŸå§‹ä»£ç åµŒå…¥ï¼ˆç”¨äºèŠ‚ç‚¹è‡ªèº«ï¼‰
            node.implementation_embedding = self._generate_embeddings(node.full_code)
                
            # æ£€ç´¢ç›¸ä¼¼å®ç°ç°‡ï¼ˆk=1ï¼Œå–æœ€ç›¸ä¼¼ï¼‰
            impl_results = self.implementation_retriever.search(query=node.full_code, k=1)
                
            if impl_results["distances"] and len(impl_results["distances"][0]) > 0:
                similarity = 1 / (1 + impl_results["distances"][0][0])  # è·ç¦»è½¬ç›¸ä¼¼åº¦
                if similarity <= self.similarity_threshold:
                    # ç›¸ä¼¼åº¦è¾¾æ ‡ï¼šåˆ†é…å·²æœ‰ç°‡IDï¼Œæ›´æ–°å…¸å‹æ¡ˆä¾‹
                    implementation_cluster_id = impl_results["ids"][0][0]
                    cluster_metadata = impl_results["metadatas"][0][0]
                    updated_node_ids = cluster_metadata["å…¸å‹æ¡ˆä¾‹"] + [node.id]
                    # é‡æ–°æ·»åŠ ç°‡ï¼ˆæ›´æ–°å…ƒæ•°æ®ï¼‰
                    self.implementation_retriever.delete_document(implementation_cluster_id)
                    self.implementation_retriever.add_cluster(
                        cluster=cluster_metadata["ç°‡ä¸­å¿ƒ"],
                        metadata={
                            "type": "implementation",
                            "ç°‡ä¸­å¿ƒ": cluster_metadata["ç°‡ä¸­å¿ƒ"],
                            "æ¨¡å¼è¯¦ç»†æ€»ç»“": cluster_metadata["æ¨¡å¼è¯¦ç»†æ€»ç»“"],
                            "å…¸å‹æ¡ˆä¾‹": updated_node_ids
                        },
                        cluster_id=implementation_cluster_id
                    )
            elif implementation_cluster_id == None:
                    # ç›¸ä¼¼åº¦ä¸è¾¾æ ‡ï¼šåˆ›å»ºæ–°ç°‡ï¼ˆç”Ÿæˆç°‡ä¸­å¿ƒå’Œæ¨¡å¼æ€»ç»“ï¼‰
                    implementation_cluster_id = str(uuid.uuid4()) 
                    # è°ƒç”¨LLMç”Ÿæˆä»£ç å®ç°çš„ç°‡æ€»ç»“
                    cluster_summary = self._generate_implementation_cluster_summary(node.full_code)
                    # ç”Ÿæˆç°‡ä¸­å¿ƒçš„æŠ½è±¡åµŒå…¥
                    cluster_embedding = self._generate_embeddings(cluster_summary["ç°‡ä¸­å¿ƒ"])
                    # å­˜å…¥æ–°ç°‡
                    self.implementation_retriever.add_cluster(
                        cluster=cluster_summary["ç°‡ä¸­å¿ƒ"],
                        metadata={
                            "type": "implementation",
                            "ç°‡ä¸­å¿ƒ": cluster_summary["ç°‡ä¸­å¿ƒ"],
                            "æ¨¡å¼è¯¦ç»†æ€»ç»“": cluster_summary["æ¨¡å¼è¯¦ç»†æ€»ç»“"],
                            "å…¸å‹æ¡ˆä¾‹": [node.id]
                        },
                        cluster_id=implementation_cluster_id
                    )
                    # å­˜å‚¨ç°‡ä¸­å¿ƒåµŒå…¥
                    node.implementation_cluster_embedding = cluster_embedding

        return modeling_cluster_id, implementation_cluster_id

    # æ–°å¢ï¼šç”Ÿæˆå»ºæ¨¡ç°‡çš„ä¸­å¿ƒæ€»ç»“å’Œæ¨¡å¼æè¿°
    def _generate_modeling_cluster_summary(self, modeling_logic: str) -> Dict[str, str]:
        """ç”¨LLMæŠ½è±¡å»ºæ¨¡é€»è¾‘ï¼Œç”Ÿæˆç°‡ä¸­å¿ƒå’Œæ¨¡å¼è¯¦ç»†æ€»ç»“"""
        prompt = f"""
        è¯·åˆ†æä»¥ä¸‹å»ºæ¨¡é€»è¾‘ï¼Œç”Ÿæˆç»“æ„åŒ–çš„ç°‡ç‰¹å¾ï¼š
        1. ç°‡ä¸­å¿ƒï¼š1å¥è¯æ€»ç»“æ ¸å¿ƒå»ºæ¨¡æ€è·¯ï¼ˆå¦‚â€œè½¦è¾†è·¯å¾„é—®é¢˜çš„ç½‘ç»œæµæ¨¡å‹â€ï¼‰ï¼›
        2. æ¨¡å¼è¯¦ç»†æ€»ç»“ï¼šåˆ†ç‚¹æè¿°é€‚ç”¨åœºæ™¯ã€æ ¸å¿ƒå˜é‡ã€å¿…é¡»çº¦æŸã€å¯é€‰æ‰©å±•ã€‚
        
        å»ºæ¨¡é€»è¾‘ï¼š{modeling_logic}
        
        è¾“å‡ºæ ¼å¼ï¼š
        {{
            "ç°‡ä¸­å¿ƒ": "æ ¸å¿ƒå»ºæ¨¡æ€è·¯æ€»ç»“",
            "æ¨¡å¼è¯¦ç»†æ€»ç»“": "é€‚ç”¨ï¼š...\\næ ¸å¿ƒï¼š...\\nå¿…é¡»åŒ…å«ï¼š...\\nå¯é€‰æ‰©å±•ï¼š..."
        }}
        """
        try:
            response = self.llm_controller.llm.get_completion(
                prompt, response_format={"type": "json_object"}
            )
            return json.loads(response)
        except Exception as e:
            logger.error(f"å»ºæ¨¡ç°‡æ€»ç»“ç”Ÿæˆå¤±è´¥ï¼š{e}")
            return {
                "ç°‡ä¸­å¿ƒ": "æœªåˆ†ç±»å»ºæ¨¡é€»è¾‘",
                "æ¨¡å¼è¯¦ç»†æ€»ç»“": f"é€‚ç”¨ï¼šæœªçŸ¥åœºæ™¯\\næ ¸å¿ƒï¼š{modeling_logic[:50]}...\\nå¿…é¡»åŒ…å«ï¼šæ— \\nå¯é€‰æ‰©å±•ï¼šæ— "
            }

    # æ–°å¢ï¼šç”Ÿæˆå®ç°ç°‡çš„ä¸­å¿ƒæ€»ç»“å’Œæ¨¡å¼æè¿°
    def _generate_implementation_cluster_summary(self, full_code: str) -> Dict[str, str]:
        """ç”¨LLMæŠ½è±¡ä»£ç å®ç°ï¼Œç”Ÿæˆç°‡ä¸­å¿ƒå’Œæ¨¡å¼è¯¦ç»†æ€»ç»“"""
        prompt = f"""
        è¯·åˆ†æä»¥ä¸‹ä»£ç å®ç°ï¼Œç”Ÿæˆç»“æ„åŒ–çš„ç°‡ç‰¹å¾ï¼š
        1. ç°‡ä¸­å¿ƒï¼š1å¥è¯æ€»ç»“ä»£ç é£æ ¼å’ŒæŠ€æœ¯æ ˆï¼ˆå¦‚â€œPython+Gurobiçš„å­—å…¸å¼å˜é‡ç®¡ç†â€ï¼‰ï¼›
        2. æ¨¡å¼è¯¦ç»†æ€»ç»“ï¼šåˆ†ç‚¹æè¿°æŠ€æœ¯æ ˆã€ä»£ç é£æ ¼ã€é€‚ç”¨è§„æ¨¡ã€æ€§èƒ½ç‰¹ç‚¹ã€‚
        
        ä»£ç å®ç°ï¼š{full_code}
        
        è¾“å‡ºæ ¼å¼ï¼š
        {{
            "ç°‡ä¸­å¿ƒ": "ä»£ç å®ç°é£æ ¼æ€»ç»“",
            "æ¨¡å¼è¯¦ç»†æ€»ç»“": "æŠ€æœ¯æ ˆï¼š...\\nä»£ç é£æ ¼ï¼š...\\né€‚ç”¨åœºæ™¯ï¼š...\\næ€§èƒ½ï¼š..."
        }}
        """
        try:
            response = self.llm_controller.llm.get_completion(
                prompt, response_format={"type": "json_object"}
            )
            return json.loads(response)
        except Exception as e:
            logger.error(f"å®ç°ç°‡æ€»ç»“ç”Ÿæˆå¤±è´¥ï¼š{e}")
            return {
                "ç°‡ä¸­å¿ƒ": "æœªåˆ†ç±»ä»£ç å®ç°",
                "æ¨¡å¼è¯¦ç»†æ€»ç»“": f"æŠ€æœ¯æ ˆï¼šæœªçŸ¥\\nä»£ç é£æ ¼ï¼š{full_code[:50]}...\\né€‚ç”¨åœºæ™¯ï¼šæœªçŸ¥\\næ€§èƒ½ï¼šæœªçŸ¥"
            }

    def add_note(self, content: str, time: str = None, **kwargs) -> str:
        """Add a new memory noteï¼šLLMè¡¥å…¨å­—æ®µâ†’åˆ†é…åŒç°‡â†’å­˜å…¥æ£€ç´¢å™¨â†’è®¡æ•°æ¼”åŒ–"""
        # Create MemoryNote without llm_controller
        if time is not None:
            kwargs['timestamp'] = time
        node = MemoryNode(**kwargs)

        # ğŸ”§ LLM Analysis Enhancement: Auto-generate attributes using LLM if they are empty or default values
        needs_analysis = (
            node.problem_description == "General Problem" or  # problem_description is empty
            node.modeling_logic == "General Modeling Logic" or  # modeling_logic is default value
            node.key_constraint_snippets == "General Key Constraint Snippets" or  # key_constraint_snippets is default value
            node.full_code == "General Full Code"  # full_code is default value
        )
        
        if needs_analysis:
            
            try:
                # ç”¨LLMåˆ†æå†…å®¹ï¼Œæå–MemoryNodeæ ¸å¿ƒå­—æ®µ
                analysis = self.analyze_content(content)

                # Only update attributes that are not provided or have default values
                if node.problem_description == "General Problem":
                    node.problem_description = analysis["description"]
                if node.modeling_logic == "General Modeling Logic":
                    node.modeling_logic = analysis["modeling"]
                if node.key_constraint_snippets == "General Key Constraint Snippets":
                    node.key_constraint_snippets = analysis["implementation"]
                if node.full_code == "General Full Code":
                    node.full_code = analysis["code"]
            
            except Exception as e:
                print(f"Warning: LLM analysis failed, using default values: {e}")

        #logger.info(f"å»ºæ¨¡é€»è¾‘ï¼š{node.modeling_logic}")
        #logger.info(f"å®Œæ•´ä»£ç ï¼š{node.full_code}")

        # Step 3ï¼šä¸ºèŠ‚ç‚¹åˆ†é…åŒç°‡ï¼ˆå»ºæ¨¡ç°‡+å®ç°ç°‡ï¼‰
        node.modeling_cluster_id, node.implementation_cluster_id = self._assign_cluster(node)
        logger.info(f"æ–°èŠ‚ç‚¹{node.id}åˆ†é…ç°‡ï¼šå»ºæ¨¡ç°‡{node.modeling_cluster_id}ï¼Œå®ç°ç°‡{node.implementation_cluster_id}")

        # Step 4ï¼šå°†èŠ‚ç‚¹å­˜å…¥æœ¬åœ°å­˜å‚¨å’ŒChromaDB
        self.memories[node.id] = node
        
        # 4.1 å­˜å…¥å…¨é‡è®°å¿†æ£€ç´¢å™¨ï¼ˆç”¨äºå…¨é‡ç›¸ä¼¼æ£€ç´¢ï¼‰
        full_metadata = {
            "id": node.id,
            "problem_description": node.problem_description,
            "modeling_cluster_id": node.modeling_cluster_id,
            "implementation_cluster_id": node.implementation_cluster_id,
            "timestamp": node.timestamp,
            "retrieval_count": node.retrieval_count,
            "status": node.status
        }
        self.full_retriever.add_cluster(
            cluster=f"é—®é¢˜ï¼š{node.problem_description}\nå»ºæ¨¡ï¼š{node.modeling_logic}\nä»£ç ï¼š{node.full_code}",
            metadata=full_metadata,
            cluster_id=node.id
        )

        # Step 5ï¼šè®¡æ•°å¾…æ•´åˆèŠ‚ç‚¹ï¼Œè¾¾åˆ°é˜ˆå€¼è§¦å‘ç°‡æ•´åˆ
        self.evo_cnt += 1
        if self.evo_cnt % self.evo_threshold == 0:
            logger.info(f"å¾…æ•´åˆèŠ‚ç‚¹æ•°è¾¾{self.evo_threshold}ï¼Œè§¦å‘ç°‡æ•´åˆ")
            #self.consolidate_memories()

        return node.id
    
    def get_clusters(self, cluster_type: str = "all") -> Dict[str, Dict[str, Any]]:
        """
        åˆ—å‡ºå½“å‰ç³»ç»Ÿä¸­çš„ç°‡ä¿¡æ¯ã€‚
        Args:
            cluster_type: "model" / "implementation" / "all"
        Returns:
            dict: {
                "modeling": { cluster_id: {"count": int, "node_ids": [...], "representative": str}, ... },
                "implementation": { ... }
            }
        """
        model_clusters: Dict[str, Dict[str, Any]] = {}
        impl_clusters: Dict[str, Dict[str, Any]] = {}

        # æŒ‰ç°æœ‰ memory èšåˆç°‡ä¿¡æ¯ï¼ˆä¸ä¾èµ–å¤–éƒ¨æ£€ç´¢å™¨æ¥å£ï¼Œä»¥ä¿è¯å¯é æ€§ï¼‰
        for node in self.memories.values():
            # å»ºæ¨¡ç°‡èšåˆ
            if node.modeling_cluster_id:
                cid = node.modeling_cluster_id
                if cid not in model_clusters:
                    model_clusters[cid] = {
                        "count": 0,
                        "node_ids": [],
                        "representative": node.modeling_logic[:300] if node.modeling_logic else ""
                    }
                model_clusters[cid]["count"] += 1
                model_clusters[cid]["node_ids"].append(node.id)

            # å®ç°ç°‡èšåˆ
            if node.implementation_cluster_id:
                cid = node.implementation_cluster_id
                if cid not in impl_clusters:
                    impl_clusters[cid] = {
                        "count": 0,
                        "node_ids": [],
                        "representative": node.full_code[:300] if node.full_code else ""
                    }
                impl_clusters[cid]["count"] += 1
                impl_clusters[cid]["node_ids"].append(node.id)

        result: Dict[str, Dict[str, Any]] = {}
        if cluster_type in ("all", "model"):
            result["modeling"] = model_clusters
        if cluster_type in ("all", "implementation"):
            result["implementation"] = impl_clusters

        return result

    def consolidate_memories(self):
        """ç°‡æ•´åˆï¼šæ›´æ–°åŒç°‡æ£€ç´¢å™¨çš„ç°‡ä¿¡æ¯ï¼ˆåˆå¹¶ç›¸ä¼¼ç°‡ã€æ›´æ–°èŠ‚ç‚¹å…³è”ï¼‰"""
        # 1. é‡ç½®å»ºæ¨¡ç°‡æ£€ç´¢å™¨ï¼Œé‡æ–°æ•´åˆæ‰€æœ‰å»ºæ¨¡é€»è¾‘
        self.model_retriever.client.reset()
        model_clusters: Dict[str, List[str]] = {}  # key: ç°‡IDï¼Œvalue: å…³è”èŠ‚ç‚¹IDåˆ—è¡¨
        
        # 2. é‡æ–°èšåˆå»ºæ¨¡ç°‡ï¼ˆæŒ‰å·²æœ‰ç°‡IDåˆ†ç»„ï¼‰
        for node in self.memories.values():
            if not node.modeling_cluster_id:
                continue
            if node.modeling_cluster_id not in model_clusters:
                model_clusters[node.modeling_cluster_id] = []
            model_clusters[node.modeling_cluster_id].append(node.id)
        
        # 3. é‡æ–°å­˜å…¥å»ºæ¨¡ç°‡ï¼ˆæ›´æ–°å…³è”èŠ‚ç‚¹åˆ—è¡¨ï¼‰
        for cluster_id, node_ids in model_clusters.items():
            # æ‰¾åˆ°ç°‡çš„ä»£è¡¨æ€§å»ºæ¨¡é€»è¾‘ï¼ˆå–ç¬¬ä¸€ä¸ªèŠ‚ç‚¹çš„å»ºæ¨¡é€»è¾‘ï¼‰
            representative_node = self.memories[node_ids[0]]
            self.model_retriever.add_cluster(
                cluster=representative_node.modeling_logic,
                metadata={"type": "modeling", "related_node_ids": node_ids},
                cluster_id=cluster_id
            )

        # 4. å®ç°ç°‡æ•´åˆï¼ˆé€»è¾‘åŒå»ºæ¨¡ç°‡ï¼‰
        self.implementation_retriever.client.reset()
        impl_clusters: Dict[str, List[str]] = {}
        for node in self.memories.values():
            if not node.implementation_cluster_id:
                continue
            if node.implementation_cluster_id not in impl_clusters:
                impl_clusters[node.implementation_cluster_id] = []
            impl_clusters[node.implementation_cluster_id].append(node.id)
        
        for cluster_id, node_ids in impl_clusters.items():
            representative_node = self.memories[node_ids[0]]
            self.implementation_retriever.add_cluster(
                cluster=representative_node.full_code,
                metadata={"type": "implementation", "related_node_ids": node_ids},
                cluster_id=cluster_id
            )

        # 5. æ›´æ–°æ‰€æœ‰èŠ‚ç‚¹çŠ¶æ€ä¸ºâ€œå·²æ•´åˆâ€
        for node in self.memories.values():
            node.status = "integrated"
            node.evolution_history.append(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] å®Œæˆç°‡æ•´åˆ")

        # é‡ç½®å¾…æ•´åˆè®¡æ•°å™¨
        self.evo_cnt = 0
        logger.info("åŒç°‡æ•´åˆå®Œæˆï¼Œæ‰€æœ‰èŠ‚ç‚¹çŠ¶æ€æ›´æ–°ä¸ºintegrated")
    
    def read(self, memory_id: str) -> Optional[MemoryNode]:
        """Retrieve a memory note by its ID.
        
        Args:
            memory_id (str): ID of the memory to retrieve

        Returns:
            MemoryNode if found, None otherwise
        """
        return self.memories.get(memory_id)

    def find_related_memories(self, query: str, k: int = 5, cluster_type: str = "all") -> Tuple[str, List[str]]:
        """æ£€ç´¢ç›¸ä¼¼è®°å¿†ï¼šæ”¯æŒå…¨é‡/å»ºæ¨¡ç°‡/å®ç°ç°‡æ£€ç´¢"""
        if not self.memories:
            return "", []

        try:
            # é€‰æ‹©æ£€ç´¢å™¨ï¼ˆall: å…¨é‡ï¼Œmodel: å»ºæ¨¡ç°‡ï¼Œimplementation: å®ç°ç°‡ï¼‰
            if cluster_type == "model":
                results = self.model_retriever.search(query=query, k=k)
                # å»ºæ¨¡ç°‡è¿”å›çš„æ˜¯ç°‡IDï¼Œéœ€æ˜ å°„åˆ°å…³è”çš„èŠ‚ç‚¹ID
                node_ids = []
                for i, cluster_id in enumerate(results["ids"][0]):
                    metadata = results["metadatas"][0][i]
                    node_ids.extend(metadata.get("related_node_ids", []))
                results["ids"][0] = node_ids[:k]  # å–å‰kä¸ªèŠ‚ç‚¹ID
            elif cluster_type == "implementation":
                results = self.implementation_retriever.search(query=query, k=k)
                # å®ç°ç°‡åŒç†ï¼Œæ˜ å°„åˆ°èŠ‚ç‚¹ID
                node_ids = []
                for i, cluster_id in enumerate(results["ids"][0]):
                    metadata = results["metadatas"][0][i]
                    node_ids.extend(metadata.get("related_node_ids", []))
                results["ids"][0] = node_ids[:k]
            else:
                results = self.full_retriever.search(query=query, k=k)

            # æ ¼å¼åŒ–æ£€ç´¢ç»“æœ
            memory_str = ""
            related_ids = []
            for i, node_id in enumerate(results["ids"][0]):
                node = self.memories.get(node_id)
                if not node:
                    continue
                # æ›´æ–°æ£€ç´¢æ¬¡æ•°
                node.retrieval_count += 1
                self.memories[node_id] = node  # ä¿å­˜æ›´æ–°åçš„æ£€ç´¢æ¬¡æ•°
                
                # æ ¼å¼åŒ–è¾“å‡ºä¿¡æ¯
                memory_str += (
                    f"è®°å¿†ID: {node.id}\n"
                    f"é—®é¢˜æè¿°: {node.problem_description[:100]}...\n"
                    f"å»ºæ¨¡é€»è¾‘: {node.modeling_logic[:100]}...\n"
                    f"æ‰€å±å»ºæ¨¡ç°‡: {node.modeling_cluster_id}\n"
                    f"æ‰€å±å®ç°ç°‡: {node.implementation_cluster_id}\n"
                    f"æ£€ç´¢æ¬¡æ•°: {node.retrieval_count}\n"
                    f"-------------------------\n"
                )
                related_ids.append(node_id)
            
            return memory_str, related_ids
        except Exception as e:
            logger.error(f"æ£€ç´¢ç›¸ä¼¼è®°å¿†å¤±è´¥: {str(e)}")
            return "", []

    def find_related_memories_raw(self, query: str, k: int = 5) -> str:
        """åŸå§‹æ ¼å¼æ£€ç´¢ç»“æœï¼ˆåŒ…å«å…³è”èŠ‚ç‚¹ä¿¡æ¯ï¼‰"""
        if not self.memories:
            return ""
            
        # å…ˆè·å–åŸºç¡€æ£€ç´¢ç»“æœ
        _, related_ids = self.find_related_memories(query, k=k)
        memory_str = ""
        
        for node_id in related_ids[:k]:
            node = self.memories.get(node_id)
            if not node:
                continue
                
            # æ·»åŠ ä¸»èŠ‚ç‚¹ä¿¡æ¯
            memory_str += (
                f"æ—¶é—´: {node.timestamp}\n"
                f"é—®é¢˜: {node.problem_description}\n"
                f"å»ºæ¨¡: {node.modeling_logic[:150]}\n"
                f"ä»£ç ç‰‡æ®µ: {node.key_constraint_snippets[:100]}\n"
                f"-------------------------\n"
            )
            
            # æ·»åŠ å…³è”èŠ‚ç‚¹ï¼ˆé€šè¿‡linkså­—æ®µï¼‰
            for link_id in node.links[:2]:  # æ¯ä¸ªèŠ‚ç‚¹æœ€å¤šæ˜¾ç¤º2ä¸ªå…³è”èŠ‚ç‚¹
                link_node = self.memories.get(link_id)
                if link_node:
                    memory_str += (
                        f"å…³è”è®°å¿†ID: {link_id}\n"
                        f"å…³è”é—®é¢˜: {link_node.problem_description[:100]}\n"
                        f"-------------------------\n"
                    )
        
        return memory_str

    def read(self, memory_id: str) -> Optional[MemoryNode]:
        """é€šè¿‡IDè¯»å–è®°å¿†èŠ‚ç‚¹"""
        node = self.memories.get(memory_id)
        if node:
            # è¯»å–æ—¶æ›´æ–°æœ€åè®¿é—®æ—¶é—´ï¼ˆæ‰©å±•å­—æ®µï¼Œéœ€åœ¨MemoryNodeä¸­æ·»åŠ ï¼‰
            node.last_accessed = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            self.memories[memory_id] = node
        return node

    def update(self, memory_id: str, **kwargs) -> bool:
        """æ›´æ–°è®°å¿†èŠ‚ç‚¹å­—æ®µï¼ˆæ”¯æŒé—®é¢˜æè¿°ã€å»ºæ¨¡é€»è¾‘ç­‰æ ¸å¿ƒå­—æ®µï¼‰"""
        if memory_id not in self.memories:
            logger.warning(f"æ›´æ–°å¤±è´¥ï¼šè®°å¿†èŠ‚ç‚¹{memory_id}ä¸å­˜åœ¨")
            return False
            
        node = self.memories[memory_id]
        # è®°å½•åŸå§‹ç°‡IDï¼ˆç”¨äºåç»­åˆ¤æ–­æ˜¯å¦éœ€è¦é‡æ–°åˆ†é…ç°‡ï¼‰
        old_model_cluster_id = node.modeling_cluster_id
        old_impl_cluster_id = node.implementation_cluster_id
        
        # æ›´æ–°å­—æ®µï¼ˆä»…æ›´æ–°MemoryNodeä¸­å­˜åœ¨çš„å±æ€§ï¼‰
        for key, value in kwargs.items():
            if hasattr(node, key):
                setattr(node, key, value)
                logger.info(f"èŠ‚ç‚¹{memory_id}æ›´æ–°å­—æ®µ: {key}")
        
        # è‹¥å»ºæ¨¡é€»è¾‘æˆ–ä»£ç å˜æ›´ï¼Œéœ€é‡æ–°ç”ŸæˆåµŒå…¥å¹¶åˆ†é…ç°‡
        if "modeling_logic" in kwargs or "full_code" in kwargs:
            new_model_cluster_id, new_impl_cluster_id = self._assign_cluster(node)
            node.modeling_cluster_id = new_model_cluster_id
            node.implementation_cluster_id = new_impl_cluster_id
            logger.info(f"èŠ‚ç‚¹{memory_id}ç°‡ä¿¡æ¯æ›´æ–°ï¼šå»ºæ¨¡ç°‡{new_model_cluster_id}ï¼Œå®ç°ç°‡{new_impl_cluster_id}")
        
        # æ›´æ–°ChromaDBä¸­çš„å…¨é‡è®°å¿†
        full_metadata = {
            "id": node.id,
            "problem_description": node.problem_description,
            "modeling_cluster_id": node.modeling_cluster_id,
            "implementation_cluster_id": node.implementation_cluster_id,
            "timestamp": node.timestamp,
            "retrieval_count": node.retrieval_count,
            "status": node.status
        }
        self.full_retriever.delete_document(memory_id)
        self.full_retriever.add_cluster(
            cluster=f"é—®é¢˜ï¼š{node.problem_description}\nå»ºæ¨¡ï¼š{node.modeling_logic}\nä»£ç ï¼š{node.full_code}",
            metadata=full_metadata,
            cluster_id=memory_id
        )
        
        # è®°å½•æ¼”åŒ–å†å²
        node.evolution_history.append(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] æ‰‹åŠ¨æ›´æ–°èŠ‚ç‚¹")
        self.memories[memory_id] = node
        return True

    def delete(self, memory_id: str) -> bool:
        """åˆ é™¤è®°å¿†èŠ‚ç‚¹ï¼ˆåŒæ­¥åˆ é™¤åŒç°‡å…³è”ï¼‰"""
        if memory_id not in self.memories:
            return False
            
        node = self.memories[memory_id]
        # 1. ä»åŒç°‡ä¸­ç§»é™¤èŠ‚ç‚¹å…³è”ï¼ˆç®€åŒ–å¤„ç†ï¼šæ ‡è®°ç°‡éœ€é‡æ–°æ•´åˆï¼‰
        if node.modeling_cluster_id:
            logger.info(f"èŠ‚ç‚¹{memory_id}ä»å»ºæ¨¡ç°‡{node.modeling_cluster_id}ç§»é™¤")
        if node.implementation_cluster_id:
            logger.info(f"èŠ‚ç‚¹{memory_id}ä»å®ç°ç°‡{node.implementation_cluster_id}ç§»é™¤")
        
        # 2. ä»ChromaDBåˆ é™¤
        self.full_retriever.delete_document(memory_id)
        
        # 3. ä»æœ¬åœ°å­˜å‚¨åˆ é™¤
        del self.memories[memory_id]
        logger.info(f"èŠ‚ç‚¹{memory_id}å·²å®Œå…¨åˆ é™¤")
        return True

    def process_memory(self, node: MemoryNode) -> Tuple[bool, MemoryNode]:
        """è®°å¿†æ¼”åŒ–å¤„ç†ï¼šé€šè¿‡LLMåˆ¤æ–­æ˜¯å¦ä¸ç›¸ä¼¼èŠ‚ç‚¹æ•´åˆ"""
        if not self.memories or len(self.memories) == 1:  # åªæœ‰å½“å‰èŠ‚ç‚¹æ—¶æ— éœ€æ¼”åŒ–
            return False, node
            
        try:
            # 1. è·å–æœ€ç›¸ä¼¼çš„5ä¸ªèŠ‚ç‚¹ä½œä¸ºé‚»å±…
            neighbors_text, neighbor_ids = self.find_related_memories(
                query=node.problem_description, 
                k=5, 
                cluster_type="all"
            )
            if not neighbors_text or not neighbor_ids:
                return False, node
                
            # 2. æ„é€ LLMæ¼”åŒ–å†³ç­–æç¤º
            prompt = self._evolution_system_prompt.format(
                content=node.problem_description,
                context=node.modeling_logic,
                keywords=node.key_constraint_snippets[:50],  # ç”¨å…³é”®çº¦æŸä½œä¸ºå…³é”®è¯
                nearest_neighbors_memories=neighbors_text,
                neighbor_number=len(neighbor_ids)
            )
            
            # 3. è°ƒç”¨LLMè·å–æ¼”åŒ–å†³ç­–
            response = self.llm_controller.llm.get_completion(
                prompt,
                response_format={
                    "type": "json_schema",
                    "json_schema": {
                        "name": "evolution_decision",
                        "schema": {
                            "type": "object",
                            "properties": {
                                "should_evolve": {"type": "boolean"},
                                "actions": {"type": "array", "items": {"type": "string"}},
                                "suggested_connections": {"type": "array", "items": {"type": "string"}},
                                "new_context_neighborhood": {"type": "array", "items": {"type": "string"}},
                                "tags_to_update": {"type": "array", "items": {"type": "string"}},
                                "new_tags_neighborhood": {"type": "array", "items": {"type": "array", "items": {"type": "string"}}}
                            },
                            "required": ["should_evolve", "actions", "suggested_connections"],
                            "additionalProperties": False
                        },
                        "strict": True
                    }
                }
            )
            response_json = json.loads(response)
            should_evolve = response_json["should_evolve"]
            
            if not should_evolve:
                return False, node
                
            # 4. æ‰§è¡Œæ¼”åŒ–åŠ¨ä½œ
            actions = response_json["actions"]
            for action in actions:
                if action == "strengthen":
                    # å¼ºåŒ–å…³è”ï¼šæ·»åŠ å»ºè®®çš„èŠ‚ç‚¹é“¾æ¥
                    node.links.extend([
                        link_id for link_id in response_json["suggested_connections"] 
                        if link_id in self.memories and link_id not in node.links
                    ])
                    node.evolution_history.append(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] å¼ºåŒ–å…³è”ï¼š{node.links}")
                    
                elif action == "update_neighbor":
                    # æ›´æ–°é‚»å±…èŠ‚ç‚¹çš„ä¸Šä¸‹æ–‡å’Œæ ‡ç­¾ï¼ˆæ­¤å¤„ç®€åŒ–ä¸ºæ›´æ–°å»ºæ¨¡é€»è¾‘ï¼‰
                    new_contexts = response_json.get("new_context_neighborhood", [])
                    for i, neighbor_id in enumerate(neighbor_ids[:len(new_contexts)]):
                        neighbor_node = self.memories.get(neighbor_id)
                        if neighbor_node:
                            neighbor_node.modeling_logic = new_contexts[i]  # æ›´æ–°å»ºæ¨¡é€»è¾‘
                            neighbor_node.evolution_history.append(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] è¢«èŠ‚ç‚¹{node.id}æ›´æ–°å»ºæ¨¡é€»è¾‘")
                            self.memories[neighbor_id] = neighbor_node  # ä¿å­˜æ›´æ–°
            
            return True, node
            
        except Exception as e:
            logger.error(f"è®°å¿†æ¼”åŒ–å¤„ç†å¤±è´¥: {str(e)}")
            return False, node

    def search(self, query: str, k: int = 5, cluster_type: str = "all") -> List[Dict[str, Any]]:
        """ç»“æ„åŒ–æ£€ç´¢æ¥å£ï¼šè¿”å›åŒ…å«èŠ‚ç‚¹è¯¦æƒ…çš„å­—å…¸åˆ—è¡¨"""
        _, related_ids = self.find_related_memories(query, k=k, cluster_type=cluster_type)
        results = []
        
        for node_id in related_ids[:k]:
            node = self.memories.get(node_id)
            if not node:
                continue
                
            results.append({
                "id": node.id,
                "problem_description": node.problem_description,
                "modeling_logic": node.modeling_logic,
                "full_code": node.full_code[:200] + "...",  # æˆªæ–­é•¿ä»£ç 
                "modeling_cluster_id": node.modeling_cluster_id,
                "implementation_cluster_id": node.implementation_cluster_id,
                "retrieval_count": node.retrieval_count,
                "timestamp": node.timestamp,
                "status": node.status
            })
        
        return results